{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Revised and improvised by Thouseef Syed on [cAInvas](https://cainvas.ai-tech.systems/accounts/login/) platform developed by [AITS](https://ai-techsystems.com/) : An AIoT company known for deployment of ML on EDGE devices\n",
    "\n",
    "# Social Distance Detection using Single Shot Detector\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is Single Shot Detection?\n",
    "\n",
    "SSD has two components: a backbone model and SSD head. Backbone model usually is a pre-trained image classification network as a feature extractor. This is typically a network like ResNet trained on ImageNet from which the final fully connected classification layer has been removed. We are thus left with a deep neural network that is able to extract semantic meaning from the input image while preserving the spatial structure of the image albeit at a lower resolution.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<p><img align=\"left\" width=\"950\" src=\"https://cainvas-static.s3.amazonaws.com/media/user_data/txs180022.tsyed/ssd1.png\"  hspace=\"70px\" vspace=\"0px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Instead of using sliding window, SSD divides the image using a grid and have each grid cell be responsible for detecting objects in that region of the image. Detection objects simply means predicting the class and location of an object within that region. If no object is present, we consider it as the background class and the location is ignored. For instance, we could use a 4x4 grid in the example below. Each grid cell is able to output the position and shape of the object it contains.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aspect Ratio\n",
    "\n",
    "When observed, not all objects are square in shape. Some are longer and some are wider, by varying degrees. The SSD architecture allows pre-defined aspect ratios of the anchor boxes to account for this. The ratios parameter can be used to specify the different aspect ratios of the anchor boxes associates with each grid cell at each zoom/scale level.\n",
    "\n",
    "<p><img align=\"left\" width=\"250\" src=\"https://cainvas-static.s3.amazonaws.com/media/user_data/txs180022.tsyed/aspect_ratio-min.png\"  hspace=\"70px\" vspace=\"0px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE: Execute the code as a .py file and not iypnb\n",
    "\n",
    "To run the program, install the necessary packages and execute the lines below in the terminal:\n",
    "\n",
    "1. pip install opencv_python==4.2.0.32\n",
    "2. python social_distance_detection_SSD.py --prototxt SSD_MobileNet_prototxt.txt --model SSD_MobileNet.caffemodel --labels class_labels.txt --output test_video.mp4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import argparse\n",
    "import sys\n",
    "import cv2\n",
    "from math import pow, sqrt\n",
    "\n",
    "\n",
    "# Parse the arguments from command line\n",
    "arg = argparse.ArgumentParser(description='Social distance detection')\n",
    "\n",
    "arg.add_argument('-v', '--video', type = str, default = '', help = 'Video file path. If no path is given, video is captured using device.')\n",
    "\n",
    "arg.add_argument('-m', '--model', required = False, help = \"SSD_MobileNet.caffemodel\")\n",
    "\n",
    "arg.add_argument('-p', '--prototxt', required = False, help = 'SSD_MobileNet_prototxt.txt')\n",
    "\n",
    "arg.add_argument('-l', '--labels', required = False, help = 'class_labels.txt')\n",
    "\n",
    "arg.add_argument('-c', '--confidence', type = float, default = 0.2, help='Set confidence for detecting objects')\n",
    "\n",
    "arg.add_argument(\"-o\", \"--output\", type=str, default=\"\", help=\"path to (optional) output video file\")\n",
    "\n",
    "\n",
    "args = vars(arg.parse_args())\n",
    "\n",
    "\n",
    "labels = [line.strip() for line in open(args['labels'])]\n",
    "\n",
    "# Generate random bounding box bounding_box_color for each label\n",
    "bounding_box_color = np.random.uniform(0, 255, size=(len(labels), 3))\n",
    "\n",
    "\n",
    "# Load model\n",
    "print(\"\\nLoading model...\\n\")\n",
    "network = cv2.dnn.readNetFromCaffe(args[\"prototxt\"], args[\"model\"])\n",
    "\n",
    "print(\"\\nStreaming video using device...\\n\")\n",
    "\n",
    "\n",
    "# Capture video from file or through device\n",
    "if args['video']:\n",
    "    cap = cv2.VideoCapture(args['video'])\n",
    "else:\n",
    "    cap = cv2.VideoCapture(\"example.mp4\")\n",
    "writer = None\n",
    "\n",
    "\n",
    "frame_no = 0\n",
    "\n",
    "while cap.isOpened():\n",
    "\n",
    "    frame_no = frame_no+1\n",
    "\n",
    "    # Capture one frame after another\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    (h, w) = frame.shape[:2]\n",
    "\n",
    "    # Resize the frame to suite the model requirements. Resize the frame to 300X300 pixels\n",
    "    blob = cv2.dnn.blobFromImage(cv2.resize(frame, (300, 300)), 0.007843, (300, 300), 127.5)\n",
    "\n",
    "    network.setInput(blob)\n",
    "    detections = network.forward()\n",
    "\n",
    "    pos_dict = dict()\n",
    "    coordinates = dict()\n",
    "\n",
    "    # Focal length\n",
    "    F = 615\n",
    "\n",
    "    for i in range(detections.shape[2]):\n",
    "\n",
    "        confidence = detections[0, 0, i, 2]\n",
    "\n",
    "        if confidence > args[\"confidence\"]:\n",
    "\n",
    "            class_id = int(detections[0, 0, i, 1])\n",
    "\n",
    "            box = detections[0, 0, i, 3:7] * np.array([w, h, w, h])\n",
    "            (startX, startY, endX, endY) = box.astype(\"int\")\n",
    "\n",
    "            # Filtering only persons detected in the frame. Class Id of 'person' is 15\n",
    "            if class_id == 15.00:\n",
    "\n",
    "                \n",
    "                label = \"{}: {:.2f}%\".format(labels[class_id], confidence * 100)\n",
    "                print(\"{}\".format(label))\n",
    "                \n",
    "                # Draw bounding box for the object\n",
    "                cv2.rectangle(frame, (startX, startY), (endX, endY), bounding_box_color[class_id], 2)\n",
    "\n",
    "\n",
    "\n",
    "                coordinates[i] = (startX, startY, endX, endY)\n",
    "\n",
    "                # Mid point of bounding box\n",
    "                x_mid = round((startX+endX)/2,4)\n",
    "                y_mid = round((startY+endY)/2,4)\n",
    "\n",
    "                height = round(endY-startY,4)\n",
    "\n",
    "                # Distance from camera based on triangle similarity\n",
    "                distance = (165 * F)/height\n",
    "                print(\"Distance(cm):{dist}\\n\".format(dist=distance))\n",
    "\n",
    "                # Mid-point of bounding boxes (in cm) based on triangle similarity technique\n",
    "                x_mid_cm = (x_mid * distance) / F\n",
    "                y_mid_cm = (y_mid * distance) / F\n",
    "                pos_dict[i] = (x_mid_cm,y_mid_cm,distance)\n",
    "\n",
    "                \n",
    "    # Distance between every object detected in a frame\n",
    "    close_objects = set()\n",
    "    for i in pos_dict.keys():\n",
    "        for j in pos_dict.keys():\n",
    "            if i < j:\n",
    "                dist = sqrt(pow(pos_dict[i][0]-pos_dict[j][0],2) + pow(pos_dict[i][1]-pos_dict[j][1],2) + pow(pos_dict[i][2]-pos_dict[j][2],2))\n",
    "\n",
    "                # Check if distance less than 2 metres or 200 centimetres\n",
    "                if dist < 200:\n",
    "                    close_objects.add(i)\n",
    "                    close_objects.add(j)\n",
    "                    \n",
    "    for i in pos_dict.keys():\n",
    "        if i in close_objects:\n",
    "            COLOR = (0,0,255)\n",
    "        else:\n",
    "            COLOR = (0,255,0)\n",
    "        (startX, startY, endX, endY) = coordinates[i]\n",
    "        \n",
    "        \n",
    "        \n",
    "        cv2.rectangle(frame, (startX, startY), (endX, endY), COLOR, 2)\n",
    "        y = startY - 15 if startY - 15 > 15 else startY + 15\n",
    "        # Convert cms to feet\n",
    "        cv2.putText(frame, 'Depth: {i} ft'.format(i=round(pos_dict[i][2]/30.48,4)), (startX, y),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.5, COLOR, 2)\n",
    "    \n",
    "    \n",
    "    #cv2.namedWindow('Frame',cv2.WINDOW_NORMAL)\n",
    "    \n",
    "#################################################################################\n",
    "    print(\"Saving output file...........\")\n",
    "### To save video\n",
    "    if args[\"output\"] != \"\" and writer is None:\n",
    "            fourcc = cv2.VideoWriter_fourcc(*\"MJPG\")\n",
    "            writer = cv2.VideoWriter(args[\"output\"], fourcc, 25, (frame.shape[1], frame.shape[0]), True)\n",
    "        # if the video writer is not None, write the frame to the output\n",
    "        # video file\n",
    "    if writer is not None:\n",
    "        writer.write(frame)\n",
    "        \n",
    "#### Ctrl+c to exit the loop    \n",
    "\n",
    "# Clean\n",
    "cap.release()\n",
    "out.release()\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
